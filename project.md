[Max] **Option C: Full Streaming Implementation Plan** üîß

Architecture Overview

```
Python Client (ESP32 Simulator)
    ‚Üì WebSocket (wss://gateway:18789/cheeko/stream)
    ‚Üì Audio chunks (PCM 16kHz, 20ms frames)
OpenClaw Gateway WebSocket Endpoint
    ‚îú‚îÄ‚Üí Deepgram/Groq STT (streaming)
    ‚îú‚îÄ‚Üí Claude/GPT (streaming)
    ‚îî‚îÄ‚Üí ElevenLabs/Deepgram TTS (streaming)
    ‚Üì Audio chunks back
Python Client (playback simulation)
```
**Key Feature:** Pipeline parallelism ‚Äî TTS starts as soon as first LLM tokens arrive


Phase 1: Protocol Design (Day 1)

Message Schema

**Client ‚Üí Server (Audio Input)**

```
{
  "type": "audio_chunk",
  "deviceId": "cheeko-sim-001",
  "sessionId": "uuid-v4",
  "seq": 42,
  "timestamp": 1739371234567,
  "audio": {
    "data": "<base64-encoded-pcm>",
    "format": "pcm16",
    "sampleRate": 16000,
    "channels": 1,
    "durationMs": 20
  }
}
```
**Client ‚Üí Server (Control)**

```
{
  "type": "speech_end",
  "deviceId": "cheeko-sim-001",
  "sessionId": "uuid-v4",
  "timestamp": 1739371234567
}

{
  "type": "cancel",
  "deviceId": "cheeko-sim-001",
  "sessionId": "uuid-v4"
}
```
**Server ‚Üí Client (Transcript)**

```
{
  "type": "transcript",
  "partial": false,
  "text": "What's the weather like today?",
  "confidence": 0.95,
  "timestamp": 1739371234567
}
```
**Server ‚Üí Client (Audio Response)**

```
{
  "type": "audio_chunk",
  "seq": 1,
  "audio": {
    "data": "<base64-encoded-pcm>",
    "format": "pcm16",
    "sampleRate": 24000,
    "channels": 1
  },
  "metadata": {
    "totalChunks": 0,  // 0 = streaming, unknown total
    "final": false
  }
}

{
  "type": "audio_end",
  "totalChunks": 45,
  "durationMs": 2340
}
```
**Server ‚Üí Client (Status)**

```
{
  "type": "status",
  "stage": "stt" | "thinking" | "speaking",
  "message": "Processing your question..."
}
```

Phase 2: Gateway WebSocket Endpoint (Day 2-3)

File: `/gateway/endpoints/cheeko_stream.ts`

```
import WebSocket from 'ws';
import { Deepgram } from '@deepgram/sdk';
import Anthropic from '@anthropic-ai/sdk';
import ElevenLabs from 'elevenlabs-node';

interface CheekoDevice {
  deviceId: string;
  sessionId: string;
  ws: WebSocket;
  sttStream?: any;
  ttsStream?: any;
  claudeStream?: any;
}

const devices = new Map<string, CheekoDevice>();

export function setupCheekoStreamEndpoint(wss: WebSocket.Server) {
  wss.on('connection', async (ws, req) => {
    const deviceId = authenticateDevice(req); // JWT or API key
    if (!deviceId) {
      ws.close(4001, 'Unauthorized');
      return;
    }

    console.log(`[Cheeko] Device connected: ${deviceId}`);

    const device: CheekoDevice = {
      deviceId,
      sessionId: generateSessionId(),
      ws,
    };
    devices.set(deviceId, device);

    // Initialize Deepgram streaming STT
    const deepgram = new Deepgram(process.env.DEEPGRAM_API_KEY);
    const sttStream = deepgram.transcription.live({
      model: 'nova-2',
      language: 'en',
      punctuate: true,
      interim_results: false,
      endpointing: 300, // VAD silence threshold
    });

    device.sttStream = sttStream;

    // STT event handlers
    sttStream.on('transcriptReceived', async (transcript) => {
      const text = transcript.channel.alternatives[0].transcript;
      if (!text || text.trim() === '') return;

      console.log(`[STT] ${deviceId}: ${text}`);

      // Send transcript to client
      ws.send(JSON.stringify({
        type: 'transcript',
        partial: false,
        text,
        confidence: transcript.channel.alternatives[0].confidence,
        timestamp: Date.now(),
      }));

      // Send status
      ws.send(JSON.stringify({
        type: 'status',
        stage: 'thinking',
        message: 'Processing...',
      }));

      // Start Claude streaming
      await processWithClaude(device, text);
    });

    // WebSocket message handler
    ws.on('message', async (data) => {
      const msg = JSON.parse(data.toString());

      switch (msg.type) {
        case 'audio_chunk':
          // Decode base64 and pipe to STT
          const audioBuffer = Buffer.from(msg.audio.data, 'base64');
          sttStream.send(audioBuffer);
          break;

        case 'speech_end':
          // Flush STT stream
          sttStream.finish();
          break;

        case 'cancel':
          // Stop all streams
          device.claudeStream?.abort();
          device.ttsStream?.destroy();
          break;
      }
    });

    ws.on('close', () => {
      console.log(`[Cheeko] Device disconnected: ${deviceId}`);
      sttStream.finish();
      devices.delete(deviceId);
    });
  });
}

async function processWithClaude(device: CheekoDevice, userMessage: string) {
  const anthropic = new Anthropic({
    apiKey: process.env.ANTHROPIC_API_KEY,
  });

  // Get session history from OpenClaw
  const history = await getSessionHistory(device.sessionId);

  device.ws.send(JSON.stringify({
    type: 'status',
    stage: 'speaking',
    message: 'Responding...',
  }));

  // Stream Claude response
  const stream = await anthropic.messages.stream({
    model: 'claude-sonnet-4',
    max_tokens: 1024,
    messages: [
      ...history,
      { role: 'user', content: userMessage },
    ],
    system: 'You are Cheeko, a friendly AI companion for kids. Keep responses concise (1-2 sentences for simple questions).',
  });

  device.claudeStream = stream;

  let fullResponse = '';
  let sentenceBuffer = '';

  // Process Claude chunks and stream to TTS
  for await (const chunk of stream) {
    if (chunk.type === 'content_block_delta' && chunk.delta.type === 'text_delta') {
      const text = chunk.delta.text;
      fullResponse += text;
      sentenceBuffer += text;

      // Stream to TTS when we have a complete sentence
      if (isSentenceComplete(sentenceBuffer)) {
        await streamToTTS(device, sentenceBuffer);
        sentenceBuffer = '';
      }
    }
  }

  // Flush remaining text
```

```
import WebSocket from 'ws';
import { Deepgram } from '@deepgram/sdk';
import Anthropic from '@anthropic-ai/sdk';
import ElevenLabs from 'elevenlabs-node';

interface CheekoDevice {
  deviceId: string;
  sessionId: string;
  ws: WebSocket;
  sttStream?: any;
  ttsStream?: any;
  claudeStream?: any;
}

const devices = new Map<string, CheekoDevice>();

export function setupCheekoStreamEndpoint(wss: WebSocket.Server) {
  wss.on('connection', async (ws, req) => {
    const deviceId = authenticateDevice(req); // JWT or API key
    if (!deviceId) {
      ws.close(4001, 'Unauthorized');
      return;
    }

    console.log(`[Cheeko] Device connected: ${deviceId}`);

    const device: CheekoDevice = {
      deviceId,
      sessionId: generateSessionId(),
      ws,
    };
    devices.set(deviceId, device);

    // Initialize Deepgram streaming STT
    const deepgram = new Deepgram(process.env.DEEPGRAM_API_KEY);
    const sttStream = deepgram.transcription.live({
      model: 'nova-2',
      language: 'en',
      punctuate: true,
      interim_results: false,
      endpointing: 300, // VAD silence threshold
    });

    device.sttStream = sttStream;

    // STT event handlers
    sttStream.on('transcriptReceived', async (transcript) => {
      const text = transcript.channel.alternatives[0].transcript;
      if (!text || text.trim() === '') return;

      console.log(`[STT] ${deviceId}: ${text}`);

      // Send transcript to client
      ws.send(JSON.stringify({
        type: 'transcript',
        partial: false,
        text,
        confidence: transcript.channel.alternatives[0].confidence,
        timestamp: Date.now(),
      }));

      // Send status
      ws.send(JSON.stringify({
        type: 'status',
        stage: 'thinking',
        message: 'Processing...',
      }));

      // Start Claude streaming
      await processWithClaude(device, text);
    });

    // WebSocket message handler
    ws.on('message', async (data) => {
      const msg = JSON.parse(data.toString());

      switch (msg.type) {
        case 'audio_chunk':
          // Decode base64 and pipe to STT
          const audioBuffer = Buffer.from(msg.audio.data, 'base64');
          sttStream.send(audioBuffer);
          break;

        case 'speech_end':
          // Flush STT stream
          sttStream.finish();
          break;

        case 'cancel':
          // Stop all streams
          device.claudeStream?.abort();
          device.ttsStream?.destroy();
          break;
      }
    });

    ws.on('close', () => {
      console.log(`[Cheeko] Device disconnected: ${deviceId}`);
      sttStream.finish();
      devices.delete(deviceId);
    });
  });
}

async function processWithClaude(device: CheekoDevice, userMessage: string) {
  const anthropic = new Anthropic({
    apiKey: process.env.ANTHROPIC_API_KEY,
  });

  // Get session history from OpenClaw
  const history = await getSessionHistory(device.sessionId);

  device.ws.send(JSON.stringify({
    type: 'status',
    stage: 'speaking',
    message: 'Responding...',
  }));

  // Stream Claude response
  const stream = await anthropic.messages.stream({
    model: 'claude-sonnet-4',
    max_tokens: 1024,
    messages: [
      ...history,
      { role: 'user', content: userMessage },
    ],
    system: 'You are Cheeko, a friendly AI companion for kids. Keep responses concise (1-2 sentences for simple questions).',
  });

  device.claudeStream = stream;

  let fullResponse = '';
  let sentenceBuffer = '';

  // Process Claude chunks and stream to TTS
  for await (const chunk of stream) {
    if (chunk.type === 'content_block_delta' && chunk.delta.type === 'text_delta') {
      const text = chunk.delta.text;
      fullResponse += text;
      sentenceBuffer += text;

      // Stream to TTS when we have a complete sentence
      if (isSentenceComplete(sentenceBuffer)) {
        await streamToTTS(device, sentenceBuffer);
        sentenceBuffer = '';
      }
    }
  }

  // Flush remaining text
```

```
 if (sentenceBuffer.trim()) {
    await streamToTTS(device, sentenceBuffer);
  }

  // Save to session history
  await saveToSession(device.sessionId, userMessage, fullResponse);

  // Signal end
  device.ws.send(JSON.stringify({
    type: 'audio_end',
    timestamp: Date.now(),
  }));
}

async function streamToTTS(device: CheekoDevice, text: string) {
  const elevenlabs = new ElevenLabs({
    apiKey: process.env.ELEVENLABS_API_KEY,
  });

  // ElevenLabs streaming
  const audioStream = await elevenlabs.textToSpeech({
    voice_id: 'pNInz6obpgDQGcFmaJgB', // Kid-friendly voice
    text,
    model_id: 'eleven_turbo_v2', // Fastest model
    output_format: 'pcm_24000',
  });

  let seq = 0;
  for await (const chunk of audioStream) {
    device.ws.send(JSON.stringify({
      type: 'audio_chunk',
      seq: seq++,
      audio: {
        data: chunk.toString('base64'),
        format: 'pcm16',
        sampleRate: 24000,
        channels: 1,
      },
      metadata: { final: false },
    }));
  }
}

function isSentenceComplete(text: string): boolean {
  // Check for sentence endings
  return /[.!?]\s*$/.test(text.trim());
}

function authenticateDevice(req: any): string | null {
  const token = req.headers['x-device-token'];
  // Validate JWT or API key
  return 'cheeko-sim-001'; // Placeholder
}

function generateSessionId(): string {
  return `session-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
}

async function getSessionHistory(sessionId: string): Promise<any[]> {
  // Query OpenClaw session storage
  return [];
}

async function saveToSession(sessionId: string, user: string, assistant: string) {
  // Save to OpenClaw session storage
}
```

Phase 3: Python Client Simulator (Day 1)

File: `cheeko_client_simulator.py`

```
#!/usr/bin/env python3
"""
Cheeko ESP32 Simulator - Python WebSocket Client
Tests full streaming pipeline with microphone input
"""

import asyncio
import websockets
import json
import base64
import pyaudio
import wave
import time
from pathlib import Path

# Audio config (matches ESP32 I2S)
SAMPLE_RATE = 16000
CHANNELS = 1
CHUNK_MS = 20  # 20ms chunks
CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_MS / 1000)  # 320 samples

# Gateway config
GATEWAY_URL = "ws://localhost:18789/cheeko/stream"
DEVICE_TOKEN = "test-device-001"

class CheekoSimulator:
    def __init__(self):
        self.ws = None
        self.audio = pyaudio.PyAudio()
        self.input_stream = None
        self.output_stream = None
        self.device_id = "cheeko-sim-001"
        self.session_id = None
        self.seq = 0
        self.is_speaking = False
        
    async def connect(self):
        """Connect to OpenClaw Gateway"""
        headers = {"X-Device-Token": DEVICE_TOKEN}
        self.ws = await websockets.connect(GATEWAY_URL, extra_headers=headers)
        print(f"‚úÖ Connected to {GATEWAY_URL}")
        
    async def start_input_stream(self):
        """Start microphone capture"""
        self.input_stream = self.audio.open(
            format=pyaudio.paInt16,
            channels=CHANNELS,
            rate=SAMPLE_RATE,
            input=True,
            frames_per_buffer=CHUNK_SIZE,
        )
        print("üé§ Microphone active - speak now!")
        
    async def start_output_stream(self):
        """Start speaker playback"""
        self.output_stream = self.audio.open(
            format=pyaudio.paInt16,
            channels=CHANNELS,
            rate=24000,  # TTS output rate
            output=True,
        )
        
    async def send_audio_loop(self):
        """Capture and send audio chunks"""
        while True:
            try:
                # Read audio from mic
                audio_data = self.input_stream.read(CHUNK_SIZE, exception_on_overflow=False)
                
                # Encode to base64
                audio_b64 = base64.b64encode(audio_data).decode('utf-8')
                
                # Send to gateway
                msg = {
                    "type": "audio_chunk",
                    "deviceId": self.device_id,
                    "sessionId": self.session_id,
                    "seq": self.seq,
                    "timestamp": int(time.time() * 1000),
                    "audio": {
                        "data": audio_b64,
                        "format": "pcm16",
                        "sampleRate": SAMPLE_RATE,
                        "channels": CHANNELS,
                        "durationMs": CHUNK_MS,
                    }
                }
                
                await self.ws.send(json.dumps(msg))
                self.seq += 1
                
                await asyncio.sleep(CHUNK_MS / 1000.0)  # 20ms delay
                
            except Exception as e:
                print(f"‚ùå Send error: {e}")
                break
                
    async def receive_loop(self):
        """Receive and process responses"""
        async for message in self.ws:
            msg = json.loads(message)
            msg_type = msg.get('type')
            
            if msg_type == 'transcript':
                print(f"üìù Transcript: {msg['text']}")
                
            elif msg_type == 'status':
                stage = msg.get('stage')
                if stage == 'thinking':
                    print("ü§î Thinking...")
                elif stage == 'speaking':
                    print("üîä Speaking...")
                    self.is_speaking = True
                    
            elif msg_type == 'audio_chunk':
                # Decode and play audio
                audio_data = base64.b64decode(msg['audio']['data'])
                self.output_stream.write(audio_data)
                
            elif msg_type == 'audio_end':
                print("‚úÖ Response complete")
```

```
               self.is_speaking = False
                
    async def run(self):
        """Main loop"""
        await self.connect()
        await self.start_input_stream()
        await self.start_output_stream()
        
        # Run send and receive concurrently
        await asyncio.gather(
            self.send_audio_loop(),
            self.receive_loop(),
        )
        
    def cleanup(self):
        """Clean shutdown"""
        if self.input_stream:
            self.input_stream.stop_stream()
            self.input_stream.close()
        if self.output_stream:
            self.output_stream.stop_stream()
            self.output_stream.close()
        self.audio.terminate()
        print("üõë Stopped")

if __name__ == "__main__":
    sim = CheekoSimulator()
    try:
        asyncio.run(sim.run())
    except KeyboardInterrupt:
        print("\n‚è∏Ô∏è  Interrupted")
    finally:
        sim.cleanup()
```

Phase 4: Testing & Metrics (Day 4-5)

Test Cases

**TC-1: Single Question**

‚Ä¢ User: "What's 2 + 2?"
‚Ä¢ Expected: Response within 800ms
‚Ä¢ Measure: Time from speech_end to first audio_chunk
**TC-2: Long Response**

‚Ä¢ User: "Tell me a story about a robot"
‚Ä¢ Expected: First sentence starts <1s, streaming continues
‚Ä¢ Measure: Time to first audio vs total response time
**TC-3: Interruption**

‚Ä¢ User speaks while Cheeko is responding
‚Ä¢ Expected: Previous response cancelled, new question processed
‚Ä¢ Measure: Cancel latency <100ms
**TC-4: Network Jitter**

‚Ä¢ Simulate 100ms variable latency
‚Ä¢ Expected: Audio playback remains smooth (buffering)
‚Ä¢ Measure: Dropout count
**TC-5: Reconnection**

‚Ä¢ Disconnect mid-response
‚Ä¢ Expected: Automatic reconnect, session resumes
‚Ä¢ Measure: Recovery time <2s
Success Metrics

**Cold start latency**
‚Ä¢ Target: <1000ms
‚Ä¢ Measurement: speech_end ‚Üí first audio byte

**Streaming latency**
‚Ä¢ Target: <200ms
‚Ä¢ Measurement: LLM token ‚Üí TTS audio

**End-to-end (simple)**
‚Ä¢ Target: <800ms
‚Ä¢ Measurement: "What's 2+2?"

**End-to-end (complex)**
‚Ä¢ Target: <1500ms
‚Ä¢ Measurement: Multi-sentence response

**Audio quality**
‚Ä¢ Target: MOS >4.0
‚Ä¢ Measurement: Subjective listening test

**Reliability**
‚Ä¢ Target: 99.5%
‚Ä¢ Measurement: 1000 test conversations

Latency Breakdown (Target)

```
Speech End Detection: 50-100ms (VAD)
    ‚Üì
STT Processing: 100-200ms (Deepgram/Groq)
    ‚Üì
LLM First Token: 200-400ms (Claude Sonnet)
    ‚Üì
TTS First Chunk: 50-100ms (ElevenLabs Turbo)
    ‚Üì
Network + Playback: 50-100ms
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
TOTAL: 450-900ms ‚úÖ
```

Phase 5: Optimization (Day 6-7)

If Latency > 1s

**A. Switch to Faster Models**

‚Ä¢ STT: Groq Whisper (50-100ms vs Deepgram 200ms)
‚Ä¢ LLM: GPT-4o-mini (100-300ms vs Claude 200-400ms)
‚Ä¢ TTS: Play.ht Turbo (30ms vs ElevenLabs 50ms)
**B. Aggressive Sentence Chunking**

```
// Stream to TTS after 5-7 words, not full sentences
if (wordCount(sentenceBuffer) >= 5) {
  await streamToTTS(device, sentenceBuffer);
  sentenceBuffer = '';
}
```
**C. Pre-cache Common Responses**

```
const COMMON_RESPONSES = {
  "hello": "audio/hello.pcm",
  "how are you": "audio/how_are_you.pcm",
};
```
**D. Parallel STT + TTS Warmup**

```
// Start TTS connection while STT is processing
Promise.all([
  sttStream.process(audio),
  ttsClient.connect(),
]);
```

File Structure

```
/gateway/
  endpoints/
    cheeko_stream.ts          # WebSocket endpoint
  services/
    stt_service.ts            # Deepgram/Groq wrapper
    tts_service.ts            # ElevenLabs/PlayHT wrapper
    session_service.ts        # Session management
  middleware/
    device_auth.ts            # JWT validation
  
/clients/
  cheeko_simulator.py         # Python test client
  requirements.txt            # pyaudio, websockets
  test_suite.py               # Automated tests
  
/firmware/ (later)
  main/
    websocket_client.c        # ESP-IDF WebSocket
    audio_pipeline.c          # I2S capture/playback
```

Dependencies

**Gateway (Node.js/TypeScript)**
```
{
  "dependencies": {
    "ws": "^8.16.0",
    "@deepgram/sdk": "^3.0.0",
    "@anthropic-ai/sdk": "^0.17.0",
    "elevenlabs-node": "^1.1.0",
    "jsonwebtoken": "^9.0.2"
  }
}
```
**Python Client**

```
pip install websockets pyaudio numpy
```

Timeline

**1**
‚Ä¢ Task: Protocol design + Python client
‚Ä¢ Owner: Max + Rahul
‚Ä¢ Deliverable: Client code + spec doc

**2**
‚Ä¢ Task: Gateway WebSocket endpoint
‚Ä¢ Owner: Max
‚Ä¢ Deliverable: `/cheeko/stream` working

**3**
‚Ä¢ Task: STT/LLM/TTS integration
‚Ä¢ Owner: Max
‚Ä¢ Deliverable: Full pipeline functional

**4**
‚Ä¢ Task: Testing + latency measurement
‚Ä¢ Owner: Rahul
‚Ä¢ Deliverable: Test results, metrics

**5**
‚Ä¢ Task: Optimization (if needed)
‚Ä¢ Owner: Max + Rahul
‚Ä¢ Deliverable: <1s latency achieved

**6**
‚Ä¢ Task: Edge case handling
‚Ä¢ Owner: Both
‚Ä¢ Deliverable: Reconnect, errors, etc

**7**
‚Ä¢ Task: Documentation + handoff
‚Ä¢ Owner: Both
‚Ä¢ Deliverable: Ready for firmware port


Next Steps

**Rahul:**

1. Install dependencies: `pip install websockets pyaudio`
2. Test microphone: `python -c "import pyaudio; p=pyaudio.PyAudio(); print(p.get_default_input_device_info())"`
3. I'll push the Python simulator code tonight
**Max:**

1. Set up Gateway WebSocket endpoint (tonight)
2. Integrate Deepgram/ElevenLabs
3. Deploy to test server
**Target:** First voice test by **tomorrow evening (Feb 13)**
